{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# !pip show pyspark"
      ],
      "metadata": {
        "collapsed": true,
        "id": "73goMx0XK8FW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!apt-get update\n",
        "!apt-get install openjdk-8-jdk -y"
      ],
      "metadata": {
        "collapsed": true,
        "id": "1PdYchPlRt8K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!rm -rf spark-3.2.0-bin-hadoop3.2\n",
        "!rm -rf spark-3.0.3-bin-hadoop3.2\n",
        "!rm -f spark-3.*.tgz"
      ],
      "metadata": {
        "id": "YWENS1o_h8BT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!java -version"
      ],
      "metadata": {
        "id": "X1ERS1griCqL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. Install Dependencies\n",
        "# Install JDK 8 (still necessary for the JVM)\n",
        "!apt-get install openjdk-8-jdk-headless -qq > /dev/null\n",
        "\n",
        "# Install the correct PySpark version (3.5.1) and a utility library\n",
        "# The 'pyspark' package contains all necessary Java binaries, simplifying setup.\n",
        "!pip install -q pyspark==3.5.1\n",
        "\n",
        "# 2. Set Java Home\n",
        "import os\n",
        "from pyspark.sql import SparkSession\n",
        "\n",
        "# Set JAVA_HOME, which is often the final piece needed for the JVM\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
        "\n",
        "# 3. Create the SparkSession\n",
        "# PySpark will now use its internal libraries and findspark is not strictly needed.\n",
        "spark = SparkSession.builder \\\n",
        "    .appName(\"ColabSparkAuto\") \\\n",
        "    .master(\"local[*]\") \\\n",
        "    .config(\"spark.driver.memory\", \"6g\") \\\n",
        "    .getOrCreate()\n",
        "\n",
        "print(\"\\n---\")\n",
        "print(\"Spark initialized successfully! Spark Version:\", spark.version)"
      ],
      "metadata": {
        "id": "_rUWSFp1iHhz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "# Set environment variables for Java\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
        "os.environ[\"SPARK_HOME\"] = \"/content/spark-3.1.2-bin-hadoop3.2\"\n",
        "os.environ[\"PATH\"] = os.environ[\"JAVA_HOME\"] + \"/bin:\" + os.environ[\"PATH\"]\n"
      ],
      "metadata": {
        "id": "c3DDViTSSauO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import broadcast\n",
        "\n",
        "category_data = [\n",
        "    (1, \"Electronics\"),\n",
        "    (2, \"Apparel\"),\n",
        "    (3, \"Home Goods\"),\n",
        "    (4, \"Media\"),\n",
        "    (5, \"Books\")\n",
        "]\n",
        "category_cols = [\"category_id\", \"category_name\"]\n",
        "small_dim_df = spark.createDataFrame(category_data, category_cols)\n",
        "\n",
        "# Large DF\n",
        "large_transaction_df = spark.range(1000000) \\\n",
        "    .withColumnRenamed(\"id\",\"transaction_id\") \\\n",
        "    .withColumn(\"category_id\", (col(\"transaction_id\") % 5) + 1) # Add IDs 1 to 5\n",
        "\n",
        "print(f\"Small DF Count: {small_dim_df.count()}, Large DF Count: {large_transaction_df.count()}\")\n",
        "\n",
        "start_time = time.time()\n",
        "united_small_large_df = large_transaction_df.join(\n",
        "    broadcast(small_dim_df),\n",
        "    on = \"category_id\",\n",
        "    how = \"inner\"\n",
        ")\n",
        "united_small_large_df.show(10)\n",
        "time_broadcast_join = time.time() - start_time\n",
        "\n",
        "print(f\"time with broadcast join: {time_broadcast_join}\")"
      ],
      "metadata": {
        "id": "WS_qQh-CBEvY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "487cac70-de5e-4337-8aae-0d2baefb56a7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Small DF Count: 5, Large DF Count: 1000000\n",
            "+-----------+--------------+-------------+\n",
            "|category_id|transaction_id|category_name|\n",
            "+-----------+--------------+-------------+\n",
            "|          1|             0|  Electronics|\n",
            "|          2|             1|      Apparel|\n",
            "|          3|             2|   Home Goods|\n",
            "|          4|             3|        Media|\n",
            "|          5|             4|        Books|\n",
            "|          1|             5|  Electronics|\n",
            "|          2|             6|      Apparel|\n",
            "|          3|             7|   Home Goods|\n",
            "|          4|             8|        Media|\n",
            "|          5|             9|        Books|\n",
            "+-----------+--------------+-------------+\n",
            "only showing top 10 rows\n",
            "\n",
            "time with broadcast join: 0.5385096073150635\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}